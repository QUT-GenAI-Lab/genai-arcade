---
layout: tag
tag: values
permalink: /tag/values/
title: Values
slug: values
before: short-term-memory
after: align-me
---

# Values

If *Data* is about what a model can know, then *Values* are about what it should do with that knowledge. 

GenAI doesn’t have beliefs or intentions. But it is built by people, and people have belives and make choices. Choices about what the model should say, what it shouldn’t, and how it should respond in different situations. These choices are called alignment strategies, or sometimes safety systems.

There are different ways to shape a model’s behavior:
- *Excluding certain data* during training, so the model never sees it.
- *Aligning the model* through fine-tuning on examples of preferred behavior.
- *Adding guardrails* in the form of rules and filters that guide or block certain outputs at runtime.

Each of these methods has strengths and weaknesses. None are perfect. Excluding data can create gaps. Alignment can introduce bias. Guardrails can be too rigid or too vague. And all of them reflect human judgment, which is always situated, contested, and evolving. There is real power in deciding what is acceptable and what is not, in drawing the boundaries of conversation, shaping what can be said, and determining whose values are reflected in the system.

---

## What counts as “safe” or “appropriate” can vary across cultures, communities, and contexts:
- A topic that’s sensitive in one place might be ordinary in another.
- A joke that’s funny to some might be offensive to others.
- A question that seems neutral might carry deep historical or political weight.

So when a model avoids a topic, or responds cautiously, it’s not being evasive. It’s following a set of instructions designed to balance usefulness with responsibility.

Understanding this helps us see the model not just as a technical system, but as a social one — shaped by ethics, policy, and the ongoing negotiation of what it means to be helpful, respectful, and fair.
